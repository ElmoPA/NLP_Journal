This repository is what I use to understand NLP, mainly attention and transformers. Basically, I run through Andrej Kaparthy Courses on NLP.

Some of the papers I used are:
[Attention is All you need](https://arxiv.org/abs/1706.03762) 
[Neural language translation by learning align and translate](https://www.google.com/search?q=neural+language+translation+by+learning+align+and+translate&oq=neural+language+translation+by+learning+align+and+translate&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORigATIHCAEQIRigATIHCAIQIRigATIHCAMQIRigATIKCAQQIRgWGB0YHjIKCAUQIRgWGB0YHjIKCAYQIRgWGB0YHjIKCAcQIRgWGB0YHjIKCAgQIRgWGB0YHjIKCAkQIRgWGB0YHtIBCDcxNjNqMGoxqAIAsAIA&sourceid=chrome&ie=UTF-8)

Resources I used:
[Andrej Kaparthy Lecture on Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1363)
[Andrej Kaparthy NLP Series](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

The GPT model was trained on Kaggle using 2xT100 provided for free. [Notebook Link](https://www.kaggle.com/code/elmoaphiwetsaa/gptclass/edit)