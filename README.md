This repository is what I use to understand NLP, mainly attention and transformers.

In this repository, I experiment with concepts from various papers to solidify my understanding of fundamental NLP algorithm, attention, and transformers.
[Attention is All you need](https://arxiv.org/abs/1706.03762) 
[Neural language translation by learning align and translate](https://www.google.com/search?q=neural+language+translation+by+learning+align+and+translate&oq=neural+language+translation+by+learning+align+and+translate&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORigATIHCAEQIRigATIHCAIQIRigATIHCAMQIRigATIKCAQQIRgWGB0YHjIKCAUQIRgWGB0YHjIKCAYQIRgWGB0YHjIKCAcQIRgWGB0YHjIKCAgQIRgWGB0YHjIKCAkQIRgWGB0YHtIBCDcxNjNqMGoxqAIAsAIA&sourceid=chrome&ie=UTF-8)

Resources I used:
[Andrej Kaparthy Lecture on Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1363)